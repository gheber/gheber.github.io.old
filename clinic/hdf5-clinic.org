#+TITLE: Call the Doctor - HDF(5) Clinic
#+STARTUP: overview
#+OPTIONS: num:nil H:2
#+SETUPFILE: ~/org/theme-readtheorg.setup

* Clinic 2021-03-16
** Your questions
** Last week's highlights
*** Announcements
- [[https://forum.hdfgroup.org/t/webinar-announcement-hermes-a-distributed-buffering-system-for-heterogeneous-storage-hierarchies/8269][Webinar Announcement: Hermes – A Distributed Buffering System for
  Heterogeneous Storage Hierarchies]]
- [[https://forum.hdfgroup.org/t/hsds-version-0-6-3-released/8270][HSDS version 0.6.3 released]]
  - Grab the latest image on Docker Hub =hdfgroup/hsds:v0.6.3=!
*** Forum
**** [[https://forum.hdfgroup.org/t/multithreaded-writing-to-a-single-file-in-c/8264][Multithreaded writing to a single file in C++ ]]
- Beware of non-thread-safe wrappers or language bindings!
- Compiling the C library with =--enable-threadsafe= is only the first step
**** [[https://forum.hdfgroup.org/t/reference-manual-in-doxygen/8230][Reference Manual in Doxygen]]
- Making good progress; see [[https://github.com/HDFGroup/hdf5/tree/doxygen2][GitHub]] and [[https://hdf5.io/develop/index.html][preview]]
- What do you think?
- In which order should existing content be migrated? [[https://forum.hdfgroup.org/t/reference-manual-in-doxygen/8230][Chime in!]]
  - [[https://portal.hdfgroup.org/display/knowledge][HDF Knowledge Base]]?
- Examples:
  - [[https://hdf5.io/develop/api-compat-macros.html][API Compatibility Macros]]
  - [[https://hdf5.io/develop/_t_n_m_d_c.html][Metadata Caching in HDF5]]
  - [[https://hdf5.io/develop/_f_m_t.html][File Format Specification]]
**** [[https://forum.hdfgroup.org/t/h5i-get-name-call-is-very-slow-for-hdf5-file-5-gb/8246][~H5Iget_name~ call is very slow for HDF5 file > 5 GB]]
- [[https://portal.hdfgroup.org/display/HDF5/H5I_GET_NAME][~H5Iget_nname~]] constructs /an/ HDF5 path name given an object identifier
  - _Use Case_: You are in a corner of an application where all you've got is a
    handle (identifier) and you would like to render something meaningful to
    humans.
- It's not so much the file size but the number and arrangement of objects that
  makes ~H5Iget_name~ slow
  - See the [[https://portal.hdfgroup.org/display/HDF5/h5stat][=h5stat=]] output the user provided!
- What contributes to ~H5Iget_name~ being slow?
  - The path names are not stored in an HDF5 file (except in symbolic links...)
    and are created on-demand
  - In general, HDF5 /arrangements/ are not trees, not even directed graphs, but
    directed multi-graphs
    - A node can be the target of multiple edges (including from the same source
      node)
    - Certain nodes (groups) can be source and target of an edge
- *Take-Home-Message:*Unless you are certain that your HDF5 arrangement is a
  tree, you are skating on thin ice with path names!
  - Trying to uniquely identify objects via path name is asking for trouble
    - Use addresses + file IDs (pre-HDF 1.12) or tokens (HDF 1.12+) for that!

** Tips & tricks
*** How to open an HDF5 in append mode?

To be clear, there is no =H5F*= call that behaves like an append call. But we
can mimic one as follows:

*Credits:* Werner Benger

#+begin_src C -n

hid = H5Fcreate(filename, H5F_ACC_EXCL|H5F_ACC_SWMR_WRITE, fcpl_id, fapl_id);
if (hid < 0)
  {
    hid = H5Fopen(filename, H5F_ACC_RDWR|H5F_ACC_SWMR_WRITE, fapl_id);
  }

if (hid < 0)
  // something's going on...

#+end_src

- If the file exists ~H5Fcreate~ will fail and ~H5Fopen~ with ~H5F_ACC_RDWR~
  will kick in.
  - If the file is not an HDF5 file, both will fail.
- If the file does not exist, ~H5Fcreate~ will do its job.

* Clinic 2021-03-09
** Your questions (as of 9:00 a.m. Central Time)
*** Question 1
#+begin_quote
Is there a limit on array size if I save an array as an attribute of a dataset?

In terms of the performance, is there any consequence if I save a large amount
of data into an attribute?
#+end_quote

- Size limit :: No, not in newer versions (1.8.x+) of HDF5. See [[https://portal.hdfgroup.org/pages/viewpage.action?pageId=48808714][What limits are there in HDF5?]]
  - Make sure that downstream applications can handle such attributes (i.e., use
    HDF5 1.8.x or later)
  - Remember to tell the library that you want to use the 1.8 or later file
    format via [[https://portal.hdfgroup.org/display/HDF5/H5F_SET_LIBVER_BOUNDS][~H5Fset_libverbounds~]] (e.g., set ~low~ to ~H5F_LIBVER_V18~)
  - Also keep an eye on [[https://portal.hdfgroup.org/display/HDF5/H5P_SET_ATTR_PHASE_CHANGE][~H5Pset_attr_phase_change~]] (Consider setting ~max_compact~ to 0.)
- Performance :: It depends. (...on what you mean by /performance/)
  - Attributes have a different function (from datasets) in HDF5
    - They "decorate" other objects - application metadata
  - Their values are treated as atomic units, i.e., you will always write and
    read the entire "large" value.
    - In other words, you lose partial I/O
    - Several layouts available for datasets are not supported with attributes
      - No compression

*** Question 2
#+begin_quote
Question regarding hdf5 I/O performance, compare saving data into a large array
in one dataset Vs saving data into several smaller arrays and in several
dataset. Any consequence in terms of the performance? Will there be any sweet
spot for best performance? Or any tricks to make it reading/writing faster? I
know parallel I/O but parallel I/O would need hardware support which is not
always available. So the question is about the tricks to speed up I/O without
parallel I/O.
#+end_quote

- One large dataset vs. many small datasets, which is faster? :: It depends.
  - How do you access the data?
    - Do you always write/read the entire array in the order it was written?
    - Is it WORM (write once read many)?
      - How and how frequently does it change?
  - How compressible is the data?
    - Do you need to store data at all? E.g., [[https://www.hdfgroup.org/2021/02/webinar-learn-about-data-virtualization-with-hdf5-udf-and-how-it-can-streamline-your-work-materials/][HDF5-UDF]]
  - What is performance for you and how do you measure it?
  - What percentage of total runtime does your application spend doing I/O?
  - What scalability behavior do you expect?
  - Assuming throughput is the measure, create a baseline for your target
    system, for example, via [[https://fio.readthedocs.io/en/latest/fio_doc.html][FIO]] or [[https://github.com/hpc/ior][IOR]]
    - Your goal is to saturate the I/O subsystem
    - Is this a dedicated system?
  - Which other systems do you need to support? Are you the only user? What's
    the future?
  - **What's the budget?**

** Last week's highlights
*** Announcements
**** [[https://forum.hdfgroup.org/t/ann-h5py-3-2-released/8232][h5py 3.2 release]]
- Supports the HDF5 S3 (read-only) VFD thanks to Satrajit Ghosh
  - Need to [[https://docs.h5py.org/en/stable/build.html#source-install][build h5py from source]]
- Minimum Python version 3.7
- Interesting bugfix: [[https://github.com/h5py/h5py/issues/1817][Fix reading data with a datatype of variable-length arrays of fixed length strings]]
**** HDF5 Community BOF at ECP Community BOF Days
- March 30, 3:00 p.m. (Eastern)
- [[https://exascaleproject.zoomgov.com/meeting/register/vJItf-Chrj4jH9xNnStZkPE-P84YcZ52-7s][Registration]]
*** Forum
**** [[https://forum.hdfgroup.org/t/get-object-header-size/8233][Get Object Header size]]
- The user created a compound type with 100s of fields and eventually saw this
  error:
  #+begin_example
H5Oalloc.c line 1312 in H5O__alloc(): object header message is too large
  #+end_example
- This issue was first raised (Jira-ticket HDFFV-1089 date) on  Jun 08, 2009
- **Root cause:** the size of header message data is represented in a 2 byte
  unsigned integer (see section IV.A.1.a and IV.A.1.b of the [[https://portal.hdfgroup.org/display/HDF5/File+Format+Specification][HDF5 file format spec.]])
  - Ergo, header messages, currently, cannot be larger than 64 KB.
  - Datatype information is stored in a header message (see section IV.A.2.d)
  - This can be fixed with a file format update, but it's /fallen through the
    cracks/ for over 10 years
- The customer is always right, but who needs 100s of fields in a compound type?
  - _Use Case_: You have a large record type and you always (or most of the
    time) read and write all fields together.
  - Outside this narrow use case you are bound to lose a lot of performance and
    flexibility
- You are Leaving the +American Sector+ Mainstream: not too many tools will be
  able to handle your data
- Better approach: divide-and-conquer, i.e., go w/ a group of compounds or
  individual columns
**** [[https://forum.hdfgroup.org/t/using-hdf5-in-qt-creator/8194][Using HDF5 in Qt Creator]]
- Linker can't find ~H5::FileAccPropList()~ and ~H5::FileCreatPropList()~
- Works fine in release mode, but not in debug mode
- AFAIK, we don't distribute debug libraries in binary form. Still doesn't
  explain why the user couldn't use the release binaries in a debug build,
  unless QT Creator is extra pedantic?
**** [[https://forum.hdfgroup.org/t/reference-manual-in-doxygen/8230][Reference Manual in Doxygen]]
- Making good progress; see [[https://github.com/HDFGroup/hdf5/tree/doxygen2][GitHub]] and [[https://hdf5.io/develop/index.html][preview]]
- What do you think?
- In which order should existing content be migrated? [[https://forum.hdfgroup.org/t/reference-manual-in-doxygen/8230][Chime in!]]
  - [[https://portal.hdfgroup.org/display/knowledge][HDF Knowledge Base]]?
- Examples:
  - [[https://hdf5.io/develop/api-compat-macros.html][API Compatibility Macros]]
  - [[https://hdf5.io/develop/_t_n_m_d_c.html][Metadata Caching in HDF5]]
  - [[https://hdf5.io/develop/_f_m_t.html][File Format Specification]]
**** [[https://forum.hdfgroup.org/t/h5i-get-name-call-is-very-slow-for-hdf5-file-5-gb/8246][~H5Iget_name~ call is very slow for HDF5 file > 5 GB]]
- [[https://portal.hdfgroup.org/display/HDF5/H5I_GET_NAME][~H5Iget_nname~]] constructs /an/ HDF5 path name given an object identifier
  - _Use Case_: You are in a corner of an application where all you've got is a
    handle (identifier) and you would like to render something meaningful to
    humans.
- It's not so much the file size but the number and arrangement of objects that
  makes ~H5Iget_name~ slow
  - See the [[https://portal.hdfgroup.org/display/HDF5/h5stat][=h5stat=]] output the user provided!
- What contributes to ~H5Iget_name~ being slow?
  - The path names are not stored in an HDF5 file (except in symbolic links...)
    and are created on-demand
  - In general, HDF5 /arrangements/ are not trees, not even directed graphs, but
    directed multi-graphs
    - A node can be the target of multiple edges (including from the same source
      node)
    - Certain nodes (groups) can be source and target of an edge
- *Take-Home-Message:*Unless you are certain that your HDF5 arrangement is a
  tree, you are skating on thin ice with path names!
  - Trying to uniquely identify objects via path name is asking for trouble
    - Use addresses + file IDs (pre-HDF 1.12) or tokens (HDF 1.12+) for that!

* Clinic 2021-03-02
** Your questions
*** [[https://github.com/steven-varga/h5rnd][h5rnd]]
- **Question:** How are generated HDF5 objects named?  An integer name, or can a
  randomized string be used?
  - =h5rnd= Generates a pool of random strings as link names
  - Uniform length distribution between 5 and 30 over =[a-z][A-Z]=

- **Question:** Does it create multi-dimensional datasets with a rich set of
  HDF5 datatypes?  Compound datatypes, perhaps?
  - Currently, it creates 1,000 element 1D FP64 datasets (w/ attribute)
  - RE: types - anything is possible. Budget?

- **Question:** Are named datatypes generated? If not, are these reasonable
  types of extensions for =h5rnd=?
  - Not currently, but anything is possible

*** Other questions?
- **Question:** How do these extensions fit with the general intent and
  extensibility of =h5rnd=?
  - It was written as an illustration
  - Uses an older version of H5CPP
  - Labeling could be improved
  - Dataset generation under development
  - Some enhancements in a future version

** Last week's highlights
*** Forum
**** [[https://forum.hdfgroup.org/t/external-link-access-in-parallel-hdf5-1-12-0/8219][External link access in parallel HDF5 1.12.0]]
- Can't access externally linked datasets in parallel; fine in 1.10.x and in serial
- It appears that someone encountered a known bug in the field
- Dev. claim it's fixed in =develop=, waiting for confirmation from the user

**** [[https://forum.hdfgroup.org/t/h5i-dec-ref-hangs/8104][=H5I_dec_ref= hangs]]
- [[https://portal.hdfgroup.org/display/HDF5/H5I_DEC_REF][=H5Idec_ref=]] is one of those functions that needs to be used w/ extra care
- Using =mpi4py= and =h5py=
- User provided an MWE (in Python) and, honestly, there is limited help we can
  offer (as we are neither =mpi4py= nor =h5py= experts)
- A C or C++ MWE might be the better starting point

**** [[https://forum.hdfgroup.org/t/h5diff-exits-with-1-but-doesnt-print-differences/6872][=h5diff= exits with 1 but doesn’t print differences]]
- Case of out-of-date/poor documentation
- [[https://portal.hdfgroup.org/display/HDF5/h5diff][=h5diff=]] is perhaps the most complex tool (multi-graph comparison + what does
  ~'='~ mean?)
- Writing code is the easy part
- We need to do better

**** [[https://forum.hdfgroup.org/t/independent-datasets-for-mpi-processes-progress/8202][Independent datasets for MPI processes. Progress?]]
- Need some clarification on the problem formulation
- Current status (w/ MPI) MD-modifying ops. must be collective
- On the horizon: asynchronous operations (ASYNC VOL)

**** [[https://forum.hdfgroup.org/t/writing-to-virtual-datasets/8188][Writing to virtual datasets]]
- Apparently broken when a datatype conversion (truncation!) is involved

* Clinic 2021-02-23
** Your questions
*** How to use ~H5Ocopy~ in C++ code?
- [[https://forum.hdfgroup.org/t/how-to-use-h5copy-in-c-code/8175][Forum post]]
  #+begin_quote
  sandhya.v250 (Feb 19)

  Hello Team, I want to copy few groups from one hdf5 file to hdf5 another file
  which is not yet created and this should be done inside the C++ code..can you
  please tell me how can I use this inside this tool
#+end_quote

- The [[https://portal.hdfgroup.org/display/HDF5/H5O_COPY][function in question]]  (there is also a tool called =h5copy=):
  #+begin_src C

herr_t H5Ocopy
(
 hid_t       src_loc_id,
 const char* src_name,
 hid_t       dst_loc_id,
 const char* dst_name,
 hid_t       ocpypl_id,
 hid_t       lcpl_id
);

#+end_src

- The emphasis  appears to be on C++
  - You can do this in C. It's just more boilerplate.
  - Whenever I need something C++, I turn to my colleague Steven Varga (=
    Mr. [[http://h5cpp.org/][H5CPP]])
  - He also created a nice [[https://github.com/steven-varga/h5rnd][random HDF5 file generator/tester]] (= 'Prüfer' in
    German)

*** Steven's solution (excerpt)

The full example can be downloaded from [[https://github.com/steven-varga/HDFGroup-mailinglist/tree/master/object-copy-2021-feb-18][here]].

**Basic idea:** Visit all objects in the source via ~H5Ovisit~ and invoke
~H5Ocopy~ in the callback.

#+begin_src C++ -N

#include "argparse.h"
#include <h5cpp/all>
#include <string>

herr_t ocpy_callback(hid_t src, const char *name, const H5O_info_t *info,
                     void *dst_) {
  hid_t* dst = static_cast<hid_t*>(dst_);
  int err = 0;
  switch( info->type ){
  case H5O_TYPE_GROUP:
    if(H5Lexists( *dst, name, H5P_DEFAULT) >= 0)
      err = H5Ocopy(src, name, *dst, name, H5P_DEFAULT, H5P_DEFAULT);
    break;
  case H5O_TYPE_DATASET:
    err = H5Ocopy(src, name, *dst, name, H5P_DEFAULT, H5P_DEFAULT);
    break;
  default: /*H5O_TYPE_NAMED_DATATYPE, H5O_TYPE_NTYPES, H5O_TYPE_UNKNOWN */
    ; // nop to keep compiler happy
  }
  return 0;
}

int main(int argc, char **argv)
{
  argparse::ArgumentParser arg("ocpy", "0.0.1");
  arg.add_argument("-i", "--input")
    .required().help("path to input hdf5 file");
  arg.add_argument("-s", "--source")
    .default_value(std::string("/"))
    .help("path to group within hdf5 container");
  arg.add_argument("-o", "--output").required()
    .help("the new hdf5 will be created/or opened rw");
  arg.add_argument("-d", "--destination")
    .default_value(std::string("/"))
    .help("target group");

  std::string input, output, source, destination;
  try {
    arg.parse_args(argc, argv);
    input = arg.get<std::string>("--input");
    output = arg.get<std::string>("--output");
    source = arg.get<std::string>("--source");
    destination = arg.get<std::string>("--destination");

    h5::fd_t fd_i = h5::open(input, H5F_ACC_RDONLY);
    h5::fd_t fd_o = h5::create(output, H5F_ACC_TRUNC);
    h5::gr_t dgr{H5I_UNINIT}, sgr = h5::gr_t{H5Gopen(fd_i, source.data(),
                                                     H5P_DEFAULT)};
    h5::mute();
    if( destination != "/" ){
      char * gname = destination.data();
      dgr = H5Lexists(fd_o, gname, H5P_DEFAULT) >= 0 ?
        h5::gr_t{H5Gcreate(fd_o, gname, H5P_DEFAULT, H5P_DEFAULT,
                           H5P_DEFAULT)}
        : h5::gr_t{H5Gopen(fd_i, gname, H5P_DEFAULT)};
      H5Ovisit(sgr, H5_INDEX_CRT_ORDER, H5_ITER_NATIVE, ocpy_callback, &dgr );
    } else
      H5Ovisit(sgr, H5_INDEX_CRT_ORDER, H5_ITER_NATIVE, ocpy_callback, &fd_o);
    h5::unmute();
  } catch ( const h5::error::any& e ) {
    std::cerr << e.what() << std::endl;
    std::cout << arg;
  }
  return 0;
}

#+end_src

*** Parting thoughts
- This is can be tricky business depending on how selective you want to be
- ~H5Ovisit~ visits /objects/ and does not account for dangling links, etc.
- ~H5Ocopy~'s behavior is highly customizable. Check the options & play w/
  =h5copy= to see the effect!

*** More Questions
**** Question 1
#+begin_quote
I have an unrelated question. I have 7,000 HDF5 files, each 500 MB long. When I
use them, should I open them selectively, when I need them, or is it
advantageous to make one big file, or to open virtual files? I am interested in
the speed of the different approaches.
#+end_quote

- 40 GbE connectivity
- 10 contiguously laid out Datasets per file => ~50 MB per dataset
- Always reading full datasets
- **Considerations:**
  - If you have the RAM and use all data in an "epoch" just read whole files and
    use [[https://support.hdfgroup.org/HDF5/doc/Advanced/FileImageOperations/HDF5FileImageOperations.pdf][HDF5 file images]] for "in-memory I/O."
  - You could maintain an index file /I/ which contains external links (one for
    each of the 7,000 files), and a dataset which for each external file and
    dataset contains the offset of the dataset in the file. You would keep /I/
    (small!) in memory and, for each dataset request, read the ~50MB directly
    w/o the HDF5 library. This assumes that no datatype conversion is necessary
    and you have no trouble interpreting the bytes.
  - A variation of the previous approach would be for the stub-file to contain
    HDF5 [[https://www.star.nesdis.noaa.gov/jpss/documents/HDF5_Tutorial_201509/2-3-Working%20with%20collections%20of%20HDF5%20files.pptx.pdf][/virtual datasets/]], datasets stitched together from other
    datasets. This would we a good option, if you wanted to simplify your
    application code and make everything appear as a single large HDF5
    file. It'd be important though to have that (small) stub-file on the clients
    in memory to not incur a high latency penalty.
  - Both approaches can be easily parallelized, assuming read-only access. If
    there are writers involved, it's still doable, but additional considerations
    apply.

#+begin_quote
Another question: what is the recommended way to combine Python with C++ with
C++ reading in and working on large hdf5 files that require a lot of speed.
#+end_quote

- To be honest, we ran out of time and I (GH) didn't fully grasp the question.
- Steven said something about Julia
- Henric uses Boost Python.  What about Cython?
- What's the access pattern?

  **Let's continue the discussion on the forum or come back next week!**

** Last week's highlights
*** Forum
- [[https://forum.hdfgroup.org/t/possible-bug-with-hyperslab-selection-in-1-10-7/8164][Hyperslab selection bug confirmed]]
- [[https://forum.hdfgroup.org/t/write-data-to-variable-length-string-attribute/8101][Write data to variable length string attribute]] bug is a feature =;-)=
- [[https://forum.hdfgroup.org/t/creating-a-dataset-through-external-link-does-not-allow-hdf5view-to-see-resulting-dataset-from-root-file/8148][Possible bug in HDFView]]
  - The [[https://en.wikipedia.org/wiki/Minimal_working_example][MWE]] checks out and it looks more and more like a bug in HDFView

** Appendix
*** The =h5copy= command line tool

#+begin_example
gerd@guix ~$ h5copy

usage: h5copy [OPTIONS] [OBJECTS...]
   OBJECTS
      -i, --input        input file name
      -o, --output       output file name
      -s, --source       source object name
      -d, --destination  destination object name
   OPTIONS
      -h, --help         Print a usage message and exit
      -p, --parents      No error if existing, make parent groups as needed
      -v, --verbose      Print information about OBJECTS and OPTIONS
      -V, --version      Print version number and exit
      --enable-error-stack
                  Prints messages from the HDF5 error stack as they occur.
      -f, --flag         Flag type

      Flag type is one of the following strings:

      shallow     Copy only immediate members for groups

      soft        Expand soft links into new objects

      ext         Expand external links into new objects

      ref         Copy references and any referenced objects, i.e., objects
                  that the references point to.
                    Referenced objects are copied in addition to the objects
                  specified on the command line and reference datasets are
                  populated with correct reference values. Copies of referenced
                  datasets outside the copy range specified on the command line
                  will normally have a different name from the original.
                    (Default:Without this option, reference value(s) in any
                  reference datasets are set to NULL and referenced objects are
                  not copied unless they are otherwise within the copy range
                  specified on the command line.)

      noattr      Copy object without copying attributes

      allflags    Switches all flags from the default to the non-default setting

      These flag types correspond to the following API symbols

      H5O_COPY_SHALLOW_HIERARCHY_FLAG
      H5O_COPY_EXPAND_SOFT_LINK_FLAG
      H5O_COPY_EXPAND_EXT_LINK_FLAG
      H5O_COPY_EXPAND_REFERENCE_FLAG
      H5O_COPY_WITHOUT_ATTR_FLAG
      H5O_COPY_ALL
  #+end_example

* Clinic 2021-02-16
** Your questions
** Last week's highlights
*** Events
- [[https://www.youtube.com/watch?v=7ZYZKB7DNUg][Webinar: /Learn about data virtualization with HDF5-UDF and how it can
  streamline your work/]]
- [[https://www.youtube.com/watch?v=g5h_YlvI9Aw][Recording of last week's clinic]]
*** Forum
- [[https://forum.hdfgroup.org/t/hdfql-2-3-0-release-with-excel-import-export-support/8132][Release of HDFql 2.3.0]]
  - Excel import and export (no Excel or OLE dependency!)
- [[https://forum.hdfgroup.org/t/creating-a-dataset-through-external-link-does-not-allow-hdf5view-to-see-resulting-dataset-from-root-file/8148][Possible bug in HDFView]]
  - Based on the description, it appears that external links are not properly
    traversed
  - Waiting for [[https://en.wikipedia.org/wiki/Minimal_working_example][MWE]]
- [[https://forum.hdfgroup.org/t/possible-bug-with-hyperslab-selection-in-1-10-7/8164][Possible hyperslab selection bug in 1.10.7]]
  - It appears that =H5S_SELECT_OR= is no longer "commutative", i.e., ~H5Dread~
    with =SelA OR SelB= works, but not =SelB OR SelA=
  - Wasn't there in 1.10.6
** Notes
*** What (if any) are the ACID properties of HDF5 operations?
**** Split-state
The state of an open (for RW) HDF5 file is /split/ between RAM and persistent
storage. Often the partial states will be out of sync. In the event of a
"catastrophic" failure (power outage, application crash, system crash), it is
impossible to predict what the partial state on disk will be.

#+begin_src plantuml :hidden :file hdf5-file-state.png
skinparam componentStyle rectangle

package "HDF5 File State" {
  database "Disk" {
    [Partial State 1]
  }
  cloud "RAM" {
    [Partial State 2]
  }
}
#+end_src

#+RESULTS:
[[file:hdf5-file-state.png]]

**** Non-transactional
The main reason why it is impossible to predict the outcome is that HDF5
operations are non-transactional. By 'transaction' I mean a collection of
operations (and the effects of their execution) on the physical and abstract
application state. In particular, there are no concepts of beginning a
transaction, a commit, or a roll-back. Since they are not transactional, it is
not straightforward to speak about the ACID properties of HDF5 operations.

**** File system facilities
People sometimes speak about ACID properties with respect to file system
operations. Although the HDF5 library relies on file system operations for the
implementation of HDF5 operations, the correspondence is not as direct as
might wish. For example, what appears as a single HDF5 operation to the user
often includes multiple file system operations. Several file system operations
have a certain property only at the level of a single operation, but not
multiple operations combined.

**** ACID
- Atomicity :: All changes to an HDF5 file's state must complete or fail as a
  whole unit.
  - Supported in HDF5? **No.**
  - Some file systems only support single op. atomicity, if at all.
  - A lot of HDF5 operations are /in-place/; mixed success -> impossible to
    recover
- Consistency :: An operation is a correct transformation of the HDF5 file's
  state.
  - Supported in HDF5? **Yes and No**
  - Depends on one's definition of HDF5 file/object integrity constraints
  - Assuming we are dealing with a correct program
  - Special case w/ multiple processes: Single Writer Multiple Reader
- Isolation (serialization) :: Even though operations execute concurrently, it
appears to each operation, OP, that others executed either before OP or after
  OP, but not both.
  - Supported in HDF5? **No.**
  - Depends on concurrency scenario and requires special configuration (e.g.,
    MT, MPI).
  - Time-of-check-time-of-use vulnerability
- Durability :: Once an operation completes successfully, it's changes to the
  file's state survive failure.
  - Supported in HDF5? **No.**
  - "Split brain"
  - No transaction log

* Clinic 2021-02-09
**THIS MEETING IS BEING RECORDED** and the recording will be available on The
HDF Group's [[https://www.youtube.com/channel/UCRhtsIZquL3r-zH-R-r9-tQ][YouTube channel]]. Remember to subscribe!

** Goal(s)
** This is a meeting dedicated to /your/ questions.
** In the unlikely event there aren't any
We have a few prepared topics (forum posts, announcements, etc.)
** Sometimes life deals you an HDF5 file
No question is too small. We are here to learn. All of us.

** Meeting Etiquette
** Be social, turn on your camera (if you've got one)
Talking to black boxes isn't fun.

** Raise your hand to signal a contribution (question, comment)
Mute yourself while others are speaking, be ready to participate.

** Be mindful of your "airtime"
We want to cover as many of your topics as possible. Be fair to others.

** Introduce yourself
1. Your Name
2. Your affiliation/organization/group
3. One reason why you are here today

** Use the shared Google doc for questions and code snippets
The [[https://docs.google.com/document/d/17j7MsAdNdCLxxWA_PtXv3jTgWDTqOm2uTk5jHCj8zXM/edit][link]] can be found in the chat window.

** When the 30 min. timer runs out, this meeting is over.
Continue the discussion on the [[https://forum.hdfgroup.org/][HDF Forum]] or come back next week!

** Notes
** Don't miss our next webinar about data virtualization with HDF5-UDF and how it can streamline your work
- Presented by Lucas Villa Real (IBM Research)
- Feb 12, 2021 11:00 AM in Central Time (US and Canada)
- [[https://zoom.us/meeting/register/tJEsc-iqrj4iGNJirT52ttruJ1DH2uWmFV][Sign-up link]]

** Bug-of-the-Week Award (my candidate)
- [[https://forum.hdfgroup.org/t/write-data-to-variable-length-string-attribute/8101][Write data to variable length string attribute]] by Kerim Khemraev
- [[https://jira.hdfgroup.org/browse/HDFFV-11215][Jira issue HDFFV-11215]]
- Quick demonstration
  #+begin_src C++ :tangle ./foo4k.cpp
  #include "hdf5.h"

  #include <filesystem>
  #include <iostream>
  #include <string>

  #define H5FILE_NAME "Attributes.h5"
  #define ATTR_NAME   "VarLenAttr"

  namespace fs = std::filesystem;

  int main(int argc, char *argv[])
  {
    hid_t file, attr;

    auto attr_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(attr_type, H5T_VARIABLE);
    H5Tset_cset(attr_type, H5T_CSET_UTF8);

    auto make_scalar_attr = [](auto& file, auto& attr_type)
     -> hid_t
    {
      auto attr_space  = H5Screate(H5S_SCALAR);
      auto result = H5Acreate(file, ATTR_NAME,
                              attr_type, attr_space,
                              H5P_DEFAULT, H5P_DEFAULT);
      H5Sclose(attr_space);
      return result;
    };

    if( !fs::exists(H5FILE_NAME) )
      { // If the file doesn't exist we create it &
        // add a root group attribute
        std::cout << "Creating file...\n";
        file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC,
                        H5P_DEFAULT, H5P_DEFAULT);
        attr = make_scalar_attr(file, attr_type);
      }
    else
      { // File exists: we either delete the attribute and
        // re-create it, or we just re-write it.
        std::cout << "Opening file...\n";
        file = H5Fopen(H5FILE_NAME, H5F_ACC_RDWR, H5P_DEFAULT);

  #ifndef REWRITE_ONLY
        H5Adelete(file, ATTR_NAME);
        attr = make_scalar_attr(file, attr_type);
  #else
        attr = H5Aopen(file, ATTR_NAME, H5P_DEFAULT);
  #endif
      }

    // Write or re-write the attribute
    const char* data[1] = { "Let it be λ!" };
    H5Awrite(attr, attr_type, data);

    hsize_t size;
    H5Fget_filesize(file, &size);
    std::cout << "File size: " << size << " bytes\n";

    H5Tclose(attr_type);
    H5Aclose(attr);
    H5Fclose(file);
  }
  #+end_src

** Documentation update
- Doxygen-based RM
- Remaining: a few =H5P=  calls and =H5E=
- Current version: [[https://hdf5.io/develop/modules.html]]


* COMMENT Anatomy
A few /skeletons/ to get started.

** C

#+begin_src C -r -n :tangle ctd.c :noweb no-export :results output

#include "hdf5.h"

int main(int argc, char** argv)
{
  <<main-variables>>

  <<print-library-version>>

  <<create-or-open-a-file>>

  <<release-handles>>
}

#+end_src

*** =h5dump=

#+begin_src sh :results output

h5dump -pB hello.hdf5

#+end_src

*** =<<main-variables>>=

#+begin_src C -r -n :noweb-ref main-variables

hid_t file, lcpl;

lcpl = H5Pcreate(H5P_LINK_CREATE);
H5Pset_create_intermediate_group(lcpl, 1);

#+end_src

*** =<<release-handles>>=

#+begin_src C -r -n :noweb-ref release handles

H5Fclose(file);
H5Pclose(lcpl);

#+end_src

*** =<<print-library-version>>=

#+begin_src C -r -n :noweb-ref print-library-version

{
  unsigned majnum, minnum, relnum;
  H5get_libversion(&majnum, &minnum, &relnum);
  printf("HDF5 version %u.%u.%u\n", majnum, minnum, relnum);
}

#+end_src

*** =<<create-or-open-file>>=

#+begin_src C -r -n :noweb-ref create-or-open-a-file

{
  /*
  hid_t fcpl = H5Pcreate(H5P_FILE_CREATE);
  hid_t fapl = H5Pcreate(H5P_FILE_ACCESS);
  ,*/
  file = H5Fcreate("hello.hdf5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
  /*
  H5Pclose(fapl);
  H5Pclose(fcpl);
  */
}

#+end_src

** Julia
** Python
** R

* COMMENT Helpers

#+begin_src emacs-lisp

(defun 25min-countdown ()
  (interactive)
  (setq org-timer-default-timer 25
        current-prefix-arg '(4 4)) ; C-u C-u
  (call-interactively 'org-timer-set-timer))

  #+end_src

* COMMENT Local Variables

# Local Variables:
# org-babel-C-compiler: h5cc
# org-coderef-label-format: "// (ref:%s)"
# after-save-hook: org-preview-latex-fragment
# End:
